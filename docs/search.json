[
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Augmented Testing Study",
    "section": "",
    "text": "This document contains a descriptive analysis of the effect of Augmented Testing on the relative duration of GUI-based testing. We are mainly visualizing the data as bar and violin plots."
  },
  {
    "objectID": "main.html#introduction",
    "href": "main.html#introduction",
    "title": "Augmented Testing Study",
    "section": "",
    "text": "This document contains a descriptive analysis of the effect of Augmented Testing on the relative duration of GUI-based testing. We are mainly visualizing the data as bar and violin plots."
  },
  {
    "objectID": "main.html#load-libraries",
    "href": "main.html#load-libraries",
    "title": "Augmented Testing Study",
    "section": "Load libraries",
    "text": "Load libraries\n\nsuppressPackageStartupMessages(library(caret))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(vioplot))\nsuppressPackageStartupMessages(library(tidyverse))"
  },
  {
    "objectID": "main.html#config",
    "href": "main.html#config",
    "title": "Augmented Testing Study",
    "section": "Config",
    "text": "Config\n\ncolor_main &lt;- \"#02BFC4\"\ncolor_alt &lt;- \"#F7766D\""
  },
  {
    "objectID": "main.html#import-data",
    "href": "main.html#import-data",
    "title": "Augmented Testing Study",
    "section": "Import data",
    "text": "Import data\n\ndf_raw &lt;- read.csv(file = \"../data/results.csv\", header = TRUE, sep = \",\")\n\ntc_names &lt;- c(\"TC1\", \"TC2\", \"TC3\", \"TC4\", \"TC5\", \"TC6\", \"TC7\", \"TC8\")\n\ndf &lt;- df_raw %&gt;% mutate(\n  TC_sum = TC1_seconds + TC2_seconds + TC3_seconds + TC4_seconds + TC5_seconds +\n  TC6_seconds + TC7_seconds + TC8_seconds,\n  Online_session = as.logical(Online_session))"
  },
  {
    "objectID": "main.html#seperate-two-treatments",
    "href": "main.html#seperate-two-treatments",
    "title": "Augmented Testing Study",
    "section": "Seperate two treatments",
    "text": "Seperate two treatments\n\ndf_pivot &lt;- df_raw %&gt;%\n  pivot_longer(\n    cols = c(matches(\"TC._treatment\"), matches(\"TC._seconds$\")),\n    names_to = c(\"tc\", \".value\"), names_pattern = \"TC(.)_(.*)\"\n  ) %&gt;%\n  select(\"ID\", \"tc\", \"treatment\", \"seconds\")\n\ndf_only_m &lt;- df_pivot %&gt;%\n  filter(treatment == \"M\") %&gt;%\n  pivot_wider(names_from = tc, values_from = seconds, names_prefix = \"TC\") %&gt;%\n  select(starts_with(\"TC\")) %&gt;%\n  select(order(colnames(.)))\n\ndf_only_a &lt;- df_pivot %&gt;%\n  filter(treatment == \"A\") %&gt;%\n  pivot_wider(names_from = tc, values_from = seconds, names_prefix = \"TC\") %&gt;%\n  select(starts_with(\"TC\")) %&gt;%\n  select(order(colnames(.)))"
  },
  {
    "objectID": "main.html#plot-total-test-execution-time-per-subject",
    "href": "main.html#plot-total-test-execution-time-per-subject",
    "title": "Augmented Testing Study",
    "section": "Plot total test execution time per subject",
    "text": "Plot total test execution time per subject\n\ntime_sum_bar &lt;- barplot(height = df$TC_sum, names = df$ID,\n  col = color_main,\n  horiz = TRUE, las = 1,\n  xlim = c(0, 1600),\n  xlab = \"Sum of test case duration per participant in seconds\",\n  ylab = \"Paticipant ID\"\n)\n\ntext(time_sum_bar,\n     x = df$TC_sum - 80, paste(df$TC_sum, \"sec\", sep = \" \"),\n     cex = 0.8)\n\n\n\nrec_time_sum_bar &lt;- recordPlot()"
  },
  {
    "objectID": "main.html#violin-plot-per-test-case",
    "href": "main.html#violin-plot-per-test-case",
    "title": "Augmented Testing Study",
    "section": "Violin plot per test case",
    "text": "Violin plot per test case\n\ndata &lt;- df %&gt;% select(ends_with(\"_seconds\"))\n\nvioplot(data,\n  col = color_main,\n  names = tc_names,\n  ylim = c(0, 400),\n  ylab = \"Duration in seconds\",\n  xlab = \"Distribution of duration per test case\"\n)\n\n\n\nrec_time_per_tc &lt;- recordPlot()"
  },
  {
    "objectID": "main.html#violin-plot-per-test-case-with-seperation-of-the-treatments",
    "href": "main.html#violin-plot-per-test-case-with-seperation-of-the-treatments",
    "title": "Augmented Testing Study",
    "section": "Violin plot per test case with seperation of the treatments",
    "text": "Violin plot per test case with seperation of the treatments\n\nvioplot(df_only_m,\n  side = \"left\",\n  col = color_alt,\n  ylim = c(0, 400),\n  horiz = TRUE, las = 1,\n  xlab = \"Distribution of duration per test case and treatment\",\n  ylab = \"Duration in seconds\"\n)\n\nvioplot(df_only_a,\n  side = \"right\",\n  col = color_main,\n  ylim = c(0, 400),\n  horiz = TRUE, las = 1,\n  add = TRUE\n)\n\nlegend(\"topleft\", fill = c(color_alt, color_main),\n       legend = c(\"manual\", \"AT\"), title = \"Treatment\")\n\n\n\nrec_time_per_tc_split &lt;- recordPlot()"
  },
  {
    "objectID": "main.html#mean-values-per-treatment",
    "href": "main.html#mean-values-per-treatment",
    "title": "Augmented Testing Study",
    "section": "Mean values per treatment",
    "text": "Mean values per treatment\n\ndf_mean_tc &lt;- df_raw %&gt;%\n  pivot_longer(\n    cols = c(matches(\"TC._treatment\"), matches(\"TC._seconds$\")),\n    names_to = c(\"tc\", \".value\"), names_pattern = \"TC(.)_(.*)\"\n  ) %&gt;%\n  select(\"ID\", \"tc\", \"treatment\", \"seconds\")\n\ndf_mean_tc &lt;- df_mean_tc %&gt;%\n  group_by(tc, treatment) %&gt;%\n  summarise(mean_seconds = as.integer(round(mean(seconds), 0)), .groups = \"drop\")\n\ndf_mean_tc &lt;- df_mean_tc %&gt;%\n  pivot_wider(names_from = treatment, values_from = mean_seconds) %&gt;%\n  mutate(diff_percent = as.integer(round(100 / M * (A - M), 0))) %&gt;%\n  select(tc, M, A, diff_percent)\n\n\nhead(df_mean_tc, 8)\n\n# A tibble: 8 × 4\n  tc        M     A diff_percent\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;        &lt;int&gt;\n1 1        64    73           14\n2 2        36    41           14\n3 3       256   180          -30\n4 4       246   136          -45\n5 5       101    54          -47\n6 6       101    51          -50\n7 7       196   139          -29\n8 8       222   105          -53\n\n\n\nSum of mean values per treatment\n\ntotal_a &lt;- as.integer(sum(df_mean_tc$A))\ntotal_m &lt;- as.integer(sum(df_mean_tc$M))\ntotal_diff_percent &lt;- as.integer(round(100 / total_m * (total_a - total_m), 0))\nsprintf(\"Total: %i (M) and %i (A) = %i percent\",\n        total_m, total_a, total_diff_percent)\n\n[1] \"Total: 1222 (M) and 779 (A) = -36 percent\"\n\n\n\n\nLaTeX table export\n\nsuppressPackageStartupMessages(library(xtable))\n\ndf_table &lt;- df_mean_tc %&gt;%\n  add_row(tc = \"Total\", A = total_a, M = total_m, diff_percent = total_diff_percent)\n\nprint(xtable(df_table, type = \"latex\"), include.rownames = FALSE)\n\n% latex table generated in R 4.3.1 by xtable 1.8-4 package\n% Wed Sep 20 13:10:41 2023\n\\begin{table}[ht]\n\\centering\n\\begin{tabular}{lrrr}\n  \\hline\ntc & M & A & diff\\_percent \\\\ \n  \\hline\n1 &  64 &  73 &  14 \\\\ \n  2 &  36 &  41 &  14 \\\\ \n  3 & 256 & 180 & -30 \\\\ \n  4 & 246 & 136 & -45 \\\\ \n  5 & 101 &  54 & -47 \\\\ \n  6 & 101 &  51 & -50 \\\\ \n  7 & 196 & 139 & -29 \\\\ \n  8 & 222 & 105 & -53 \\\\ \n  Total & 1222 & 779 & -36 \\\\ \n   \\hline\n\\end{tabular}\n\\end{table}"
  },
  {
    "objectID": "main.html#export-recorded-plots-as-tikz-files",
    "href": "main.html#export-recorded-plots-as-tikz-files",
    "title": "Augmented Testing Study",
    "section": "Export recorded plots as TikZ files",
    "text": "Export recorded plots as TikZ files\nTo create standalone tex files, you can add the standAlone parameter.\ntikz('standAloneExample.tex', standAlone=TRUE)\n\nsuppressPackageStartupMessages(library(tikzDevice))\n\nTest execution timer per participant\n\ntikz(\"figures/time_per_subject.tex\", width = 6, height = 4)\npar(mar = c(2, 4, 1, 1)) # Set the margin\nrec_time_sum_bar\ndev.off\n\nViolin plot of time distribution per test case\n\ntikz(\"figures/time_per_tc.tex\", width = 6.5, height = 4)\npar(mar = c(2, 4, 1, 1)) # Set the margin\nrec_time_per_tc\ndev.off\n\nViolin plot of time distribution per test case (both treatments)\n\ntikz(\"figures/time_per_tc_split.tex\", width = 6.5, height = 4)\npar(mar = c(2, 4, 1, 1)) # Set the margin\nrec_time_per_tc_split\ndev.off"
  },
  {
    "objectID": "BDA.html",
    "href": "BDA.html",
    "title": "Augmented Testing Study",
    "section": "",
    "text": "This document contains the analysis of the effect of augmented testing on the relative duration for GUI testing."
  },
  {
    "objectID": "BDA.html#causal-assumptions",
    "href": "BDA.html#causal-assumptions",
    "title": "Augmented Testing Study",
    "section": "Causal Assumptions",
    "text": "Causal Assumptions\n\nHypotheses\nWe formulate the following hypotheses based on our prior knowledge:\n\nThe use of augmented testing has an influence on the relative duration for GUI testing.\nThe size of a test case has an influence on the relative duration for GUI testing.\n\nAdditionally, we need to consider that the repeated use of the augmented testing system has an influence on the relative duration for GUI testing (learning effect).\n\n\nDirected Acyclic Graph\nWe can visualize these hypotheses in the following graph:\n\ndag &lt;- dagify(\n  dur ~ at + size + learn,\n  exposure = \"at\",\n  outcome = \"dur\",\n  labels = c(dur = \"duration_scaled\", at = \"augmented_testing\", size = \"testcase_size\", learn = \"learning_effect\"),\n  coords = list(x = c(at = 0, size = 0, learn = 0, dur = 2),\n                y = c(at = 1, size = 0, learn = -1, dur = 0))\n)\n\nggdag_status(dag,\n             use_labels = \"label\",\n             text = FALSE) +\n  guides(fill = \"none\", color = \"none\") +\n  theme_dag()"
  },
  {
    "objectID": "BDA.html#introduction",
    "href": "BDA.html#introduction",
    "title": "Augmented Testing Study",
    "section": "",
    "text": "This document contains the analysis of the effect of augmented testing on the relative duration for GUI testing."
  },
  {
    "objectID": "BDA.html#data",
    "href": "BDA.html#data",
    "title": "Augmented Testing Study",
    "section": "Data",
    "text": "Data\nFirst, load the data from the table of results.\n\ndf_raw &lt;- read.csv(file = \"../data/results.csv\", header = TRUE, sep = \",\", fileEncoding = \"UTF-8-BOM\")\n\n\nCleanup and transform data\nThe data has to be transformed into the following format\n\n\n\n\n\n\n\n\nColumn\nDescription\nType\n\n\n\n\nid\nThe identifier of a participant\nint\n\n\ntc\nThe identifier of a test case\nint\n\n\ntc_big\nSize of the test case. TRUE for bigger and FALSE for smaller test cases.\nBoolean\n\n\ntreatment\nTRUE for Augmented Testing and FALSE for manual GUI testing\nBoolean\n\n\nduration_scaled\nDuration of performing test cases (normalized)\nfloat\n\n\n\nTo achieve this, first pivot the table from wide to long format.\n\ndf &lt;- df_raw %&gt;%\n  pivot_longer(\n    cols = c(matches(\"TC._treatment\"), matches(\"TC._seconds$\")),\n    names_to = c(\"tc\", \".value\"), names_pattern = \"TC(.)_(.*)\"\n  ) %&gt;%\n  select(\"ID\", \"tc\", \"treatment\", \"seconds\") %&gt;%\n  mutate(\n    treatment = (treatment == \"A\"),\n    tc_big = (tc %in% c(3, 4, 7, 8))\n  )\n\nhead(df)\n\n# A tibble: 6 × 5\n     ID tc    treatment seconds tc_big\n  &lt;int&gt; &lt;chr&gt; &lt;lgl&gt;       &lt;int&gt; &lt;lgl&gt; \n1     1 1     FALSE          70 FALSE \n2     1 2     TRUE           35 FALSE \n3     1 3     FALSE         253 TRUE  \n4     1 4     TRUE          128 TRUE  \n5     1 5     FALSE          96 FALSE \n6     1 6     TRUE           42 FALSE \n\n\nNext, normalize the duration values: scale the duration to an interval of size 1 centered around the mean value of each test case.\n\ndf &lt;- df %&gt;%\n  group_by(tc) %&gt;%\n  mutate(\n    duration_scaled = (seconds - mean(seconds)) / (max(seconds) - min(seconds))\n  )\n\nFinally, add the learning variable: because the order of the test cases was the same for all participants, there is a potential learning effect in using AT. Hence, determine the number of previous test cases in which augmented testing has already been used to represent the learning effect.\n\n# determine the group of participants which obtained the treatment first, i.e., all with an even ID\ngroup.treatment.first &lt;- seq(2, max(df$ID), 2)\n\n# for each of the two groups, determine the \"number of previous test cases employing augmenting testing\"\ndf &lt;- df %&gt;%\n  mutate(\n    at.learned = ifelse(\n      ID %in% group.treatment.first,\n      ifelse(tc %in% c(1, 3, 5, 7), (as.integer(tc) - 1) / 2, 4),\n      ifelse(tc %in% c(4, 6, 8), (as.integer(tc) - 2) / 2, 0))\n  )"
  },
  {
    "objectID": "BDA.html#modeling",
    "href": "BDA.html#modeling",
    "title": "Augmented Testing Study",
    "section": "Modeling",
    "text": "Modeling\nNow, model the data using Bayesian data analysis.\n\nsuppressPackageStartupMessages(library(brms))\nsuppressPackageStartupMessages(library(patchwork))\nsuppressPackageStartupMessages(library(marginaleffects))\n\n\nFormula\nFirst, we define our models by (1) representing our DAG - i.e., the assumed causal relationships within our data - in a formula, and (2) selecting an appropriate distribution. In this analysis, we will compare the following models, which are further explained below:\n\n\n\nID\nDistribution\nInteraction\nLearning\nF\n\n\n\n\nm1\nGaussian\nno\nno\nf1\n\n\nm2\nGaussian\nyes\nno\nf2\n\n\nm3\nGaussian\nyes\nyes\nf3\n\n\nm4\nSkew_normal\nyes\nyes\nf3\n\n\n\nFor each formula, we can obtain a list of prior parameters which need to be defined later on via brms::get_prior.\nThe model m1 simply assumes an influence of the treatment and the test case size (tc_big) on the duration_scaled.\n\nf1 &lt;- duration_scaled ~ treatment + tc_big\n\nbrms::get_prior(f1, family = gaussian, data = df)\n\n                prior     class          coef group resp dpar nlpar lb ub\n               (flat)         b                                          \n               (flat)         b    tc_bigTRUE                            \n               (flat)         b treatmentTRUE                            \n student_t(3, 0, 2.5) Intercept                                          \n student_t(3, 0, 2.5)     sigma                                      0   \n       source\n      default\n (vectorized)\n (vectorized)\n      default\n      default\n\n\nThe model m2 additionally assumes an interaction effect between the two predictors.\n\nf2 &lt;- duration_scaled ~ treatment * tc_big\n\nbrms::get_prior(f2, family = gaussian, data = df)\n\n                prior     class                     coef group resp dpar nlpar\n               (flat)         b                                               \n               (flat)         b               tc_bigTRUE                      \n               (flat)         b            treatmentTRUE                      \n               (flat)         b treatmentTRUE:tc_bigTRUE                      \n student_t(3, 0, 2.5) Intercept                                               \n student_t(3, 0, 2.5)     sigma                                               \n lb ub       source\n            default\n       (vectorized)\n       (vectorized)\n       (vectorized)\n            default\n  0         default\n\n\nThe model m3 additionally assumes a learning effect which occurs as an interaction between the treatment and the number of test cases already processed using augmented testing (at.learned). The hypothesis is that the more often a subject has used the augmented testing system, the quicker they become (i.e., the lower the duration_scaled becomes).\n\nf3 &lt;- duration_scaled ~ treatment * tc_big + treatment * at.learned\n\nbrms::get_prior(f3, family = gaussian, data = df)\n\n                prior     class                     coef group resp dpar nlpar\n               (flat)         b                                               \n               (flat)         b               at.learned                      \n               (flat)         b               tc_bigTRUE                      \n               (flat)         b            treatmentTRUE                      \n               (flat)         b treatmentTRUE:at.learned                      \n               (flat)         b treatmentTRUE:tc_bigTRUE                      \n student_t(3, 0, 2.5) Intercept                                               \n student_t(3, 0, 2.5)     sigma                                               \n lb ub       source\n            default\n       (vectorized)\n       (vectorized)\n       (vectorized)\n       (vectorized)\n       (vectorized)\n            default\n  0         default\n\n\nThe model m4 assumes that the response variable duration_scaled does not follow a Gaussian distribution, but rather a skewed Gaussian distribution. We derive this insight from the posterior predictive checks.\n\nbrms::get_prior(f3, familiy = skew_normal, data = df)\n\n                prior     class                     coef group resp dpar nlpar\n               (flat)         b                                               \n               (flat)         b               at.learned                      \n               (flat)         b               tc_bigTRUE                      \n               (flat)         b            treatmentTRUE                      \n               (flat)         b treatmentTRUE:at.learned                      \n               (flat)         b treatmentTRUE:tc_bigTRUE                      \n student_t(3, 0, 2.5) Intercept                                               \n student_t(3, 0, 2.5)     sigma                                               \n lb ub       source\n            default\n       (vectorized)\n       (vectorized)\n       (vectorized)\n       (vectorized)\n       (vectorized)\n            default\n  0         default\n\n\nAll of these models represent our DAG but with slightly different ontological assumptions. We will compare the predictive power of these models to determine, which of these models explains the data best.\n\n\nPriors\nNext, select sensible priors. We need to set priors for the Intercept, the slope b (or “beta”), and the standard deviation sigma.\n\npriors &lt;- c(\n  prior(normal(0, 0.5), class = Intercept),\n  prior(normal(0, 0.5), class = b),\n  prior(weibull(2, 1), class = sigma)\n)\n\npriors2 &lt;- c(\n  prior(normal(0, 0.3), class = Intercept),\n  prior(normal(0, 0.3), class = b),\n  prior(weibull(2, 1), class = sigma)\n)\n\nTo assess the feasibility of the selected priors, sample from only the priors (i.e., avoid the Bayesian learning process for now) by setting sample_prior=\"only\".\n\nm1.prior &lt;- brm(\n  data = df, # provide the data\n  family = gaussian, # determine the distribution type of the response variable (here: Gaussian)\n  formula = f1, # provide the formula\n  prior = priors, # provide the priors\n  iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 4, # brms settings\n  sample_prior = \"only\", # do not train the parameters, sample only from the priors,\n  file = \"fits/m1.prior\" # save the fit in a file\n)\n\n\nm2.prior &lt;- brm(\n  data = df, family = gaussian,\n  formula = f2, prior = priors,\n  iter = 4000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4, sample_prior = \"only\", file = \"fits/m2.prior\"\n)\n\n\nm3.prior &lt;- brm(\n  data = df, family = gaussian,\n  formula = f3, prior = priors2,\n  iter = 4000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4, sample_prior = \"only\", file = \"fits/m3.prior\"\n)\n\n\nm4.prior &lt;- brm(\n  data = df, family = skew_normal,\n  formula = f3, prior = priors2,\n  iter = 4000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4, sample_prior=\"only\", file = \"fits/m4.prior\"\n)\n\nWith the competing models defined and sampled from the priors, we can perform a graphical prior predictive check by visualizing the sampled values against the actually observed values.\n\nm1.priorpc &lt;- brms::pp_check(m1.prior, type = \"dens_overlay_grouped\", ndraws = 100, group = \"treatment\") + ggplot2::ggtitle(\"M1\")\nm2.priorpc &lt;- brms::pp_check(m2.prior, type = \"dens_overlay_grouped\", ndraws = 100, group = \"treatment\") + ggplot2::ggtitle(\"M2\")\nm3.priorpc &lt;- brms::pp_check(m3.prior, type = \"dens_overlay_grouped\", ndraws = 100, group = \"treatment\") + ggplot2::ggtitle(\"M3\")\nm4.priorpc &lt;- brms::pp_check(m4.prior, type = \"dens_overlay_grouped\", ndraws = 100, group = \"treatment\") + ggplot2::ggtitle(\"M4\")\n\nm1.priorpc / m2.priorpc / m3.priorpc / m4.priorpc\n\n\n\n\nWe expect that the sampled values (\\(y_{rep}\\)) are close to the actually observed values (\\(y\\)), which confirms that the actual data is realistic in the eyes of the model given its prior believes.\n\n\nTraining\nWith sufficiently sensitive priors, we can train the model on the actual data. For this, we remove the sample_prior=\"only\" argument. Instead, we add the file=fits/m1 argument such that the trained model will be saved in the “fits” folder within the “src” folder (make sure that this folder already exists).\n\nm1 &lt;- brm(\n  data = df, family = gaussian,\n  formula = f1, prior = priors,\n  iter = 4000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4, file = \"fits/m1\"\n)\n\n\nm2 &lt;- brm(\n  data = df, family = gaussian,\n  formula = f2, prior = priors,\n  iter = 4000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4, file = \"fits/m2\"\n)\n\n\nm3 &lt;- brm(\n  data = df, family = gaussian,\n  formula = f3, prior = priors2,\n  iter = 4000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4, file = \"fits/m3\"\n)\n\n\nm4 &lt;- brm(\n  data = df, family = skew_normal,\n  formula = f3, prior = priors2,\n  iter = 4000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4, file = \"fits/m4\"\n)\n\nTo confirm that the training was successful, we can perform a graphical posterior predictive check similar to the prior predictive check, but this time sampling from the actually trained models\n\nm1.postpc &lt;- brms::pp_check(m1, type = \"dens_overlay_grouped\", ndraws = 100, group = \"treatment\") + ggplot2::ggtitle(\"M1\")\nm2.postpc &lt;- brms::pp_check(m2, type = \"dens_overlay_grouped\", ndraws = 100, group = \"treatment\") + ggplot2::ggtitle(\"M2\")\nm3.postpc &lt;- brms::pp_check(m3, type = \"dens_overlay_grouped\", ndraws = 100, group = \"treatment\") + ggplot2::ggtitle(\"M3\")\nm4.postpc &lt;- brms::pp_check(m4, type = \"dens_overlay_grouped\", ndraws = 100, group = \"treatment\") + ggplot2::ggtitle(\"M4\")\n\nm1.postpc / m2.postpc / m3.postpc / m4.postpc\n\n\n\n\nThe visualizations confirm that the parameters of the trained model imply distributions very close to the actually observed values, but only model m4 accommodates the skeweness that the values for treatment = TRUE imply.\n\n\nModel Comparison\nTo determine, which of the two formulas (and, hence, which of the two models) fit the data better, we can perform a leave-one-out comparison.\n\nm1 &lt;- add_criterion(m1, criterion = \"loo\")\nm2 &lt;- add_criterion(m2, criterion = \"loo\")\nm3 &lt;- add_criterion(m3, criterion = \"loo\")\nm4 &lt;- add_criterion(m4, criterion = \"loo\")\n\nloo_compare(m1, m2, m3, m4)\n\n   elpd_diff se_diff\nm4  0.0       0.0   \nm2 -3.0       2.7   \nm3 -3.0       3.3   \nm1 -4.2       3.2   \n\n\nThe model m4 clearly outperforms all other models, which is visible through its \\(elpd\\_diff\\) value of 0.0, while all other models perform significantly worse. Model m4 hence has the best predictive power and will be used to move forward."
  },
  {
    "objectID": "BDA.html#evaluation",
    "href": "BDA.html#evaluation",
    "title": "Augmented Testing Study",
    "section": "Evaluation",
    "text": "Evaluation\nWe can evaluate the best-fitting model to infer insights about the impact of the modeled predictors.\n\nParameters\nFirst, we can take a look at the parameters, which are the priors updated based on the observed data.\n\nsummary(m4)\n\n Family: skew_normal \n  Links: mu = identity; sigma = identity; alpha = identity \nFormula: duration_scaled ~ treatment * tc_big + treatment * at.learned \n   Data: df (Number of observations: 104) \n  Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n         total post-warmup draws = 12000\n\nPopulation-Level Effects: \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    0.02      0.06    -0.09     0.14 1.00     7616\ntreatmentTRUE               -0.07      0.08    -0.23     0.08 1.00     7048\ntc_bigTRUE                   0.18      0.07     0.04     0.31 1.00     6132\nat.learned                  -0.00      0.02    -0.04     0.04 1.00     6837\ntreatmentTRUE:tc_bigTRUE    -0.25      0.10    -0.44    -0.04 1.00     5181\ntreatmentTRUE:at.learned    -0.01      0.05    -0.11     0.07 1.00     4579\n                         Tail_ESS\nIntercept                    8843\ntreatmentTRUE                7390\ntc_bigTRUE                   7286\nat.learned                   6864\ntreatmentTRUE:tc_bigTRUE     7856\ntreatmentTRUE:at.learned     6509\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.29      0.02     0.25     0.33 1.00     8334     8457\nalpha     4.17      2.13     0.73     9.14 1.00     4483     5058\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe see the following effects:\n\nA mostly negative distribution of treatmentTRUE (“mostly negative” meaning that the most of the 95% confidence interval is negative), which means that employing augmented testing is predominantly associated with a scaled_duration value below 0, i.e., below the average for that test case.\nA strongly positive distribution of tc_bigTRUE, meaning that larger test cases tend to have a longer-than-average time for testing.\nA strongly negative distribution of treatmentTRUE:tc_bigTRUE, meaning that when a test case is large, the usage of augmented testing reduces the testing duration even stronger.\nA negligibly negative distribution of treatmentTRUE:at.learned, meaning that the the learning effect coinciding with the treatment very slightly reduces the scaled duration.\n\nThese are the interpretations of the raw parameters which do not account for all of the uncertainty which the model has picked up, but isolate the effect of the considered predictors.\n\n\nMarginal and Conditional effects\nA more sophisticated evaluation uses plots of marginal and conditional effects.\n\neff &lt;- conditional_effects(m4)\n\n\nMarginal effects\nMarginal effects represent the isolated effect of a predictor while fixing all other predictors at an average (mean or mode) level.\n\nmarginal.effect.treatment &lt;- plot(eff, plot = FALSE)[[1]]\nmarginal.effect.size &lt;- plot(eff, plot = FALSE)[[2]]\nmarginal.effect.learning &lt;- plot(eff, plot = FALSE)[[3]]\n\nmarginal.effect.treatment | marginal.effect.size | marginal.effect.learning\n\n\n\n\nIn our case, it visualizes both the effects but also the uncertainty of the impact of each individual predictor.\n\n\nConditional effects\nConditional effects visualize the interaction between two predictor variables. In the currently evaluated model, we have two interaction effects to look into:\n\nThe interaction between the treatment and the test case size (treatment * tc_big)\nThe interaction between the treatment and the learning effect (treatment * at.learned)\n\n\nconditional.effect.treatment &lt;- plot(eff, plot = FALSE)[[4]] +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Treatment\", y = \"Duration (scaled)\", fill = \"Is big test case\", color = \"Is big test case\")\n\nconditional.effect.learning &lt;- plot(eff, plot = FALSE)[[5]] +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Learing effect\", y = \"Duration (scaled)\", fill = \"AT treatment\", color = \"AT treatment\")\n\nconditional.effect.treatment | conditional.effect.learning\n\n\n\n\n\n\nquartz_off_screen \n                2 \n\n\nquartz_off_screen \n                2 \n\n\nThe visualizations show the following:\n\nLarger test cases benefit much stronger from the use of augmented testing than smaller test cases: when comparing the distributions for treatment=FALSE vs treatment=TRUE, the distance between the two means is much bigger for tc_big=TRUE than tc_big=FALSE.\nThe repeated use of augmented testing is slightly beneficial in terms of efficiency, as the effect of at.learned is more negative (i.e., reduced duration_scaled more) when treatment=TRUE than when treatment=FALSE.\n\nThe second interaction effect is fairly trivial: getting used to the augmented testing system (Scout) has a stronger effect on effectiveness when using augmented testing than when not using it. But even if this insight is trivial, it shows us that the expected learning effect is present in the data and modeling it makes the other predictors more precise by explaining at least this fraction of the effect on the response variable by the learning effect.\n\n\n\nSampling from the Posterior\nFinally, to account for all of the uncertainty of the model, we can sample from the posterior. For this, we perform the following steps:\n\nDefine a new datagrid in which we fix all predictors. In our case, we once set the treatment to TRUE and once to FALSE. In both cases, we want an even distribution of the other predictor, tc_big (hence, we set it to c(TRUE, FALSE)).\nSample from the model given both new datagrids.\nCompare the two samples by subtracting the resulting, sampled duration_scaled values from each other (diff), only keep the sign (i.e., + or -) of the difference (sign), and count, how often the sampled duration_scaled value from treatment=TRUE-sample was greater than from the treatment=FALSE-sample (table).\n\nFinally, we output those counts divided by the total amount, i.e., the percentage.\n\nposterior.draws.baseline &lt;- posterior_predict(\n  m4, newdata = datagrid(\n    model = m4,\n    treatment = FALSE,\n    tc_big = c(TRUE, FALSE)\n    ))\n\nposterior.draws.treatment &lt;- posterior_predict(\n  m4, newdata = datagrid(\n    model = m4,\n    treatment = TRUE,\n    tc_big = c(TRUE, FALSE)\n    ))\n\ndiff &lt;- posterior.draws.treatment - posterior.draws.baseline\ntab &lt;- table(sign(diff))\n\n\n(tab / sum(tab))\n\n\n       -1         1 \n0.7056667 0.2943333 \n\n\nThis means, that - accounting for all uncertainty that the model picked up and under equal distribution of test case size - the use of augmented testing on average results in shorter test duration in about 70% of all cases."
  }
]